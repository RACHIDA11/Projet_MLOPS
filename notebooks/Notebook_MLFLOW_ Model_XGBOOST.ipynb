{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b01d4-39ce-405e-9160-32beb4472d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xgb\n",
    "#from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "#import scipy.stats as stats\n",
    "#from statsmodels.graphics.tsaplots import plot_acf\n",
    "#from scipy.stats import skew, kurtosis, probplot\n",
    "#from statsmodels.stats.stattools import jarque_bera\n",
    "#from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "#from scipy.stats import shapiro\n",
    "#from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "#from lightgbm import LGBMRegressor\n",
    "#import lightgbm as lgb\n",
    "#from lightgbm import early_stopping\n",
    "#from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "#from sklearn.model_selection import cross_validate\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#import plotly.graph_objects as go\n",
    "#import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e335c-12b0-4564-9bb7-bc4799510411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow\n",
    "#!pip install --upgrade jinja2\n",
    "#!pip install --upgrade Flask\n",
    "#!pip install setuptools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d65537",
   "metadata": {},
   "source": [
    "### Plan d’Action\n",
    "Démarrer le serveur MLflow (tu l’as déjà fait)\n",
    "Créer l’expérience MLflow adaptée à ton projet\n",
    "Enregistrer les modèles et métriques XGBoost et LGBM dans MLflow\n",
    "Tester que tout fonctionne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c29f2",
   "metadata": {},
   "source": [
    "### Démarrer MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "746f06d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# starts an MLflow server locally.\n",
    "!mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f4f32",
   "metadata": {},
   "source": [
    "### Configurer MLflow pour notre projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dbd71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/21 14:17:58 INFO mlflow.tracking.fluent: Experiment with name 'Forecasting_Energie' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expérience MLflow créée: Forecasting_Energie (ID: 148799837019663067)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Connexion au serveur MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Créer une expérience MLflow spécifique à ton projet\n",
    "experiment_name = \"Forecasting_Energie\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment is None:\n",
    "    experiment_id = client.create_experiment(experiment_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"Expérience MLflow créée: {experiment_name} (ID: {experiment_id})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e5cb23-bacd-4227-ad3a-1736e71ce9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Projet_MLOPS/data/df_final_ML2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d0fab-21b8-4c7c-9eeb-31e8dea4a117",
   "metadata": {},
   "source": [
    "### Conversions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b508c65f-355a-488d-b84c-200c8b6d590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir la colonne Date en datetime\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "#Convertir la colonne Code INSEE région en object\n",
    "df['Code INSEE région'] = df['Code INSEE région'].astype('object')\n",
    "\n",
    "#Convertir la colonne Loi energie climat 2019 en integer\n",
    "\n",
    "df['Loi energie climat 2019'] = df['Loi energie climat 2019'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8ac16-9153-4d73-8508-b38dd4e0b80f",
   "metadata": {},
   "source": [
    "### Feature engineering \n",
    "\n",
    "En plus des variables temporelles ajoutés dans la partie data management (jour, mois, année, sinus, cosinus, etc), nous allons également procéder à la construction de lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0946f08-4b49-42fd-a0b7-cfafcd98e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des lags pour la consommation d'énergie jusqu'à 90 jours\n",
    "for lag in [1, 7, 30, 60, 90, 365]:  # Lags de 1, 7, 30, 60 et 90 jours\n",
    "    df[f'lag_{lag}_Consommation'] = df['Consommation (MWh)'].shift(lag)\n",
    "\n",
    "# Ajout des lags pour les variables climatiques jusqu'à 90 jours\n",
    "for lag in [1, 7, 30, 60, 90]:\n",
    "    df[f'lag_{lag}_TMin'] = df['TMin (°C)'].shift(lag)\n",
    "    df[f'lag_{lag}_TMax'] = df['TMax (°C)'].shift(lag)\n",
    "    df[f'lag_{lag}_TMoy'] = df['TMoy (°C)'].shift(lag)\n",
    "    df[f'lag_{lag}_Vitesse_vent'] = df['Vitesse du vent à 100m (m/s)'].shift(lag)\n",
    "    df[f'lag_{lag}_Rayonnement'] = df['Rayonnement solaire global (W/m2)'].shift(lag)\n",
    "\n",
    "# Supprimer les valeurs manquantes créées par les lags\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6509ffb1-a014-4a05-b7bc-d2a7ccb96c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:12: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df['Consommation_rolling_mean_7'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_7'].fillna(method='ffill')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:12: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Consommation_rolling_mean_7'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_7'].fillna(method='ffill')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:13: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df['Consommation_rolling_mean_30'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_30'].fillna(method='ffill')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Consommation_rolling_mean_30'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_30'].fillna(method='ffill')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df['Consommation_rolling_mean_90'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_90'].fillna(method='ffill')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17668\\13998302.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Consommation_rolling_mean_90'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_90'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['Code INSEE région', 'Date'])\n",
    "\n",
    "# Définir les fenêtres de rolling\n",
    "windows = [7, 30, 90]  # 7 jours, 30 jours, 90 jours\n",
    "\n",
    "for window in windows:\n",
    "    df[f'Consommation_rolling_mean_{window}'] = df.groupby('Code INSEE région')['Consommation (MWh)'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "\n",
    "#Gestion des NA : \n",
    "\n",
    "# Remplir les NaN avec la méthode de forward fill **par région**\n",
    "df['Consommation_rolling_mean_7'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_7'].fillna(method='ffill')\n",
    "df['Consommation_rolling_mean_30'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_30'].fillna(method='ffill')\n",
    "df['Consommation_rolling_mean_90'] = df.groupby('Code INSEE région')['Consommation_rolling_mean_90'].fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd480930-8c33-459a-b8ad-48cb563a6b69",
   "metadata": {},
   "source": [
    "### Gestion des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e9be32-4ab4-4e08-8b03-839e4901909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer Q1, Q3 et l'IQR pour la target\n",
    "Q1 = df['Consommation (MWh)'].quantile(0.25)\n",
    "Q3 = df['Consommation (MWh)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifier les outliers dans la consommation\n",
    "outliers = (df['Consommation (MWh)'] < (Q1 - 1.5 * IQR)) | (df['Consommation (MWh)'] > (Q3 + 1.5 * IQR))\n",
    "\n",
    "# Supprimer les lignes avec outliers dans la target\n",
    "df_cleaned = df[~outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a254bc-5539-4cb4-aef3-81677b44179e",
   "metadata": {},
   "source": [
    "### Train / test split et transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eccc88e4-84a9-42ee-b77c-742d8535ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_cleaned.copy() #copier le data set pour ne pas écraser les modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37b9936-dd27-4753-8f03-bbd8d8eeaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer la target (y) des features (X)\n",
    "df_transformed = df_transformed.sort_values(by=['Date', 'Code INSEE région']).\\\n",
    "  reset_index(drop=True)\n",
    "\n",
    "X = df_transformed[['Région','Thermique (MWh)', \n",
    "       'Hydraulique (MWh)', 'Bioénergies (MWh)', \n",
    "       'TMoy (°C)', 'Semaine_cos','Semaine_sin', 'lag_7_Consommation', 'lag_30_Consommation', 'lag_365_Consommation', \n",
    "                    'lag_7_TMoy', 'Consommation_rolling_mean_7']]\n",
    "\n",
    "y = df_transformed['Consommation (MWh)']\n",
    "\n",
    "# Split train/test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)#shuffle = False pour conserver l'ordre chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de4a4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Région', 'Thermique (MWh)', 'Hydraulique (MWh)', 'Bioénergies (MWh)',\n",
      "       'TMoy (°C)', 'Semaine_cos', 'Semaine_sin', 'lag_7_Consommation',\n",
      "       'lag_30_Consommation', 'lag_365_Consommation', 'lag_7_TMoy',\n",
      "       'Consommation_rolling_mean_7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72100e47-ea02-46a7-bc9b-1af66fc4524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#Encodage de la variable Région\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Fit uniquement sur X_train et transformer X_train\n",
    "X_train_encoded = encoder.fit_transform(X_train[['Région']])\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(['Région']), index=X_train.index)\n",
    "\n",
    "# Transformer X_test (sans refit)\n",
    "X_test_encoded = encoder.transform(X_test[['Région']])\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(['Région']), index=X_test.index)\n",
    "\n",
    "# Concaténer les colonnes encodées avec les autres features\n",
    "X_train = pd.concat([X_train.drop(columns=['Région']), X_train_encoded_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=['Région']), X_test_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8288e5b-ebac-474a-975c-f9da2553f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Région_Auvergne-Rhône-Alpes          0.452152\n",
      "Région_Île-de-France                 0.438790\n",
      "Région_Centre-Val de Loire           0.353461\n",
      "Région_Bourgogne-Franche-Comté       0.305078\n",
      "Région_Bretagne                      0.284825\n",
      "Région_Hauts-de-France               0.215121\n",
      "Région_Pays de la Loire              0.198573\n",
      "Région_Normandie                     0.188714\n",
      "Région_Grand Est                     0.132741\n",
      "Région_Nouvelle-Aquitaine            0.098058\n",
      "Région_Provence-Alpes-Côte d'Azur    0.045452\n",
      "Région_Occitanie                     0.004412\n",
      "Name: Consommation (MWh), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Concaténer X_train et y_train pour faciliter l'analyse\n",
    "df_train = X_train[['Région_Auvergne-Rhône-Alpes',\n",
    "       'Région_Bourgogne-Franche-Comté', 'Région_Bretagne',\n",
    "       'Région_Centre-Val de Loire', 'Région_Grand Est',\n",
    "       'Région_Hauts-de-France', 'Région_Normandie',\n",
    "       'Région_Nouvelle-Aquitaine', 'Région_Occitanie',\n",
    "       'Région_Pays de la Loire', \"Région_Provence-Alpes-Côte d'Azur\",\n",
    "       'Région_Île-de-France']].copy()\n",
    "df_train['Consommation (MWh)'] = y_train  \n",
    "\n",
    "# Calculer la corrélation de chaque feature avec la target\n",
    "correlations = df_train.corr()['Consommation (MWh)'].drop('Consommation (MWh)')  \n",
    "\n",
    "# Trier par ordre décroissant d'importance absolue\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "# Afficher les variables les plus corrélées\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2be1779e-4fc6-4689-b6d8-b0985f429209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garder uniquement les régions avec une corrélation supérieur à 0.30\n",
    "X_train = X_train.drop(columns=['Région_Bretagne',\n",
    "        'Région_Grand Est','Région_Hauts-de-France', 'Région_Normandie',\n",
    "       'Région_Nouvelle-Aquitaine', 'Région_Occitanie',\n",
    "       'Région_Pays de la Loire', \"Région_Provence-Alpes-Côte d'Azur\"\n",
    "       ])\n",
    "\n",
    "X_test = X_test.drop(columns=['Région_Bretagne',\n",
    "        'Région_Grand Est','Région_Hauts-de-France', 'Région_Normandie',\n",
    "       'Région_Nouvelle-Aquitaine', 'Région_Occitanie',\n",
    "       'Région_Pays de la Loire', \"Région_Provence-Alpes-Côte d'Azur\"\n",
    "       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d06abb62-0bad-4eae-8878-714fd105db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Colonnes numériques à standardiser\n",
    "normalize_cols = ['Thermique (MWh)', \n",
    "       'Hydraulique (MWh)', 'Bioénergies (MWh)', \n",
    "       'TMoy (°C)',  'lag_7_Consommation', 'lag_30_Consommation', 'lag_365_Consommation', \n",
    "                    'lag_7_TMoy', 'Consommation_rolling_mean_7']\n",
    "\n",
    "# Initialisation des scalers\n",
    "scaler_X = StandardScaler()\n",
    "\n",
    "# Standardiser X_train \n",
    "X_train[normalize_cols] = scaler_X.fit_transform(X_train[normalize_cols])\n",
    "\n",
    "# Appliquer la transformation sur  X_test (sans refit)\n",
    "X_test[normalize_cols] = scaler_X.transform(X_test[normalize_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbb7ce-9723-4216-aecf-528fa2eab847",
   "metadata": {},
   "source": [
    "### Entraînement du premier modèle ML XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd50d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Meilleurs paramètres : {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/21 14:28:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train : 2760.1172631694803\n",
      "MSE train: 15033056.535677444\n",
      "R² train : 0.9928962319347773\n",
      "MAPE train : 3.0290847926004174%\n",
      "MAE test : 4797.507268818902\n",
      "MSE test : 44922613.22361186\n",
      "R² test : 0.9760808807769817\n",
      "MAPE test : 5.899805841171993%\n",
      "🏃 View run serious-kite-968 at: http://127.0.0.1:8080/#/experiments/148799837019663067/runs/db9233bb4f784514bb004d26f5bd8077\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/148799837019663067\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Définir le modèle XGBoost\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Définir les hyperparamètres à tester avec GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 500],  # Nombre d'estimateurs\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Taux d'apprentissage\n",
    "    'max_depth': [5, 6, 7],  # Profondeur des arbres\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],  # Proportion de features pour chaque arbre\n",
    "    'gamma': [0, 0.3],  # Lutte contre l’overfitting en contrôlant les branches des arbres\n",
    "}\n",
    "\n",
    "# GridSearchCV : recherche exhaustive des meilleurs paramètres\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                           cv=3, scoring='neg_mean_absolute_error', \n",
    "                           n_jobs=-1, verbose=1)\n",
    "\n",
    "# Démarrer un suivi d'expérience avec MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Entraîner le modèle avec la recherche sur la grille\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Afficher les meilleurs paramètres trouvés\n",
    "    print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "    \n",
    "    # Log des meilleurs paramètres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Utiliser le modèle optimal pour faire des prédictions\n",
    "    best_model_xgb = grid_search.best_estimator_\n",
    "\n",
    "    # Validation croisée avec TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    mae_scores, mse_scores, mape_scores, r2_scores = [], [], [], []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_iter, X_test_iter = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_iter, y_test_iter = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Entraînement avec le meilleur modèle trouvé par GridSearchCV\n",
    "        best_model_xgb.fit(X_train_iter, y_train_iter)\n",
    "\n",
    "        # Prédictions\n",
    "        y_pred = best_model_xgb.predict(X_test_iter)\n",
    "\n",
    "        # Calcul des métriques\n",
    "        mae = mean_absolute_error(y_test_iter, y_pred)\n",
    "        mse = mean_squared_error(y_test_iter, y_pred)\n",
    "        mape = np.mean(np.abs((y_test_iter - y_pred) / y_test_iter)) * 100\n",
    "        r2 = r2_score(y_test_iter, y_pred)\n",
    "\n",
    "        mae_scores.append(mae)\n",
    "        mse_scores.append(mse)\n",
    "        mape_scores.append(mape)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    # Log des résultats de validation croisée dans MLflow\n",
    "    mlflow.log_metric(\"MAE moyen\", np.mean(mae_scores))\n",
    "    mlflow.log_metric(\"MAPE moyen\", np.mean(mape_scores))\n",
    "    mlflow.log_metric(\"MSE moyen\", np.mean(mse_scores))\n",
    "    mlflow.log_metric(\"R² moyen\", np.mean(r2_scores))\n",
    "\n",
    "    # Prédictions sur l'ensemble de test et de train\n",
    "    y_pred_train = best_model_xgb.predict(X_train)\n",
    "    y_pred_test = best_model_xgb.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques sur l'ensemble de train\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mape_train = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100  # MAPE en %\n",
    "\n",
    "    # Calcul des métriques sur l'ensemble de test\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    mape_test = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100  # MAPE en %\n",
    "\n",
    "    # Log des résultats de test et train dans MLflow\n",
    "    mlflow.log_metric(\"MAE train\", mae_train)\n",
    "    mlflow.log_metric(\"MSE train\", mse_train)\n",
    "    mlflow.log_metric(\"R² train\", r2_train)\n",
    "    mlflow.log_metric(\"MAPE train\", mape_train)\n",
    "\n",
    "    mlflow.log_metric(\"MAE test\", mae_test)\n",
    "    mlflow.log_metric(\"MSE test\", mse_test)\n",
    "    mlflow.log_metric(\"R² test\", r2_test)\n",
    "    mlflow.log_metric(\"MAPE test\", mape_test)\n",
    "\n",
    "    # Sauvegarder le modèle 'best_model_xgb' dans un fichier\n",
    "    joblib.dump(best_model_xgb, 'best_model_xgb.pkl')\n",
    "\n",
    "    # Log du modèle XGBoost dans MLflow\n",
    "    mlflow.sklearn.log_model(best_model_xgb, \"best_model\")\n",
    "\n",
    "    # Charger le modèle sauvegardé\n",
    "    best_model_xgb_final = joblib.load('best_model_xgb.pkl')\n",
    "\n",
    "    print(f\"MAE train : {mae_train}\")\n",
    "    print(f\"MSE train: {mse_train}\")\n",
    "    print(f\"R² train : {r2_train}\")\n",
    "    print(f\"MAPE train : {mape_train}%\")\n",
    "\n",
    "    print(f\"MAE test : {mae_test}\")\n",
    "    print(f\"MSE test : {mse_test}\")\n",
    "    print(f\"R² test : {r2_test}\")\n",
    "    print(f\"MAPE test : {mape_test}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bdaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
